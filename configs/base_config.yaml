base_config:
    vocab_size: 30522
    hidden_dim: 512
    num_heads: 8
    num_layers: 6
    ffn_dim: 2048
    max_len: 512

    # Embedding in event_transformer/models/embeddings.py 
    rpe: True
    rpe_class: "T5RPE"

    # APE in event_transformer/models/position.py 
    ape: False
    ape_class: "AbsolutePosition"

    # Encoder in event_transformer/models/encoder.py
    encoder_class: "" # Name of encoder class