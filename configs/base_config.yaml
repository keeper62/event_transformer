base_config:
    vocab_size: 30522
    hidden_dim: 512
    num_heads: 8
    num_layers: 6
    ffn_dim: 2048
    max_len: 512

    # Embedding in event_transformer/models/embeddings.py 
    embedding_class: "CustomEmbeddings"  # Name of embedding class
    layers:
      - "MultiHeadAttention"
      - "FeedForward"
      - "MultiHeadAttention"
      - "FeedForward"
