base_config:
    # Environment settings
    device: "auto"  # Can be "auto", "cpu", "cuda", etc.
    seed: 42        # Random seed for reproducibility

    # Model Hyperparameters
    model:
        vocab_size:
        embed_dim: 128
        num_heads: 8
        num_layers: 6
        ffn_dim: 256
        num_windows: 32          # Sliding window size for multi-head attention
        max_len: 256
        context_length: 256      # The # steps in the past as input
        prediction_steps: 32      # The # steps in the future to predict
        num_groups: 6
        dropout: 0.1
        activation: "gelu"       # Activation function (gelu, relu, silu, etc.)

        # Bias specific
        bias_injection: "attention"  # Options: "attention", "ffn"
        bias_scale: 0.1              # Float multiplier for the bias strength

    # Training Parameters
    training:
        num_epochs: 10
        batch_size: 8
        learning_rate: 0.001
        weight_decay: 0.01       # L2 regularization
        log_interval: 50         # Log every N steps
        overfit_batches: 0       # Set to >0 to overfit on small batch for debugging
        
       
        pre_norm: false       # Whether to use LayerNorm before FFN
        residual_scaling: 1.0    # Scale factor for residual connections
        
        # Checkpointing
        save_top_k: 3           # Save top k checkpoints
        monitor: "val_loss"     # Metric to monitor for checkpointing
        
        # Early stopping (optional)
        early_stopping:
            enabled: false       # Set to true to enable early stopping
            patience: 5          # Number of epochs to wait before stopping
            min_delta: 0.001     # Minimum change to qualify as improvement
        
        # Optimization
        optimizer: "adamw"       # adam, adamw, sgd, etc.
        scheduler: "reduce_on_plateau"  # Optional: reduce_on_plateau, cosine, etc.
        scheduler_params:
            factor: 0.1
            patience: 3
            mode: "min"

    # Attention Parameters
    attention:
        qkv_bias: false          # Whether to use bias in QKV projections
        out_bias: true           # Whether to use bias in output projection
        memory_efficient: true   # Use memory-efficient attention
        chunk_size: 32          # Sequence chunk size for memory efficiency
        inter_head_dim: 128       # Intermediate dimension for head collaboration
        attention_dropout: 0.1   # Dropout for attention weights

    # Positional Encoding
    position:
        ape_class: "PositionalEncoding"  # Options: PositionalEncoding, LearnableAbsolutePosition
        max_len: 64
        dropout: 0.1

    # Decoder Settings
    decoder:
        decoder_class: "MultiHeadAttention"  # Options: MultiHeadAttention, MultiHeadDenseCollaboration, CollaborativeAttention
        share_attention_weights: false       # Whether to share weights between attention layers
        gradient_checkpointing: false        # Enable memory-efficient training

    # Dataset Settings
    dataset:
        name: "xc40"
        path: "data/xc40.log"
        class: "xc40_dataset"     # Dataset class name
        drain_path: "drain3_state_xc40_old.bin"
        num_workers: 2            # DataLoader workers
        pin_memory: true          # Enable for GPU training
        persistent_workers: true  # Maintain workers between epochs
        shuffle: true             # Whether to shuffle training data

    # Logging
    logging:
        log_dir: "logs/"          # Directory for TensorBoard logs
        experiment_name: "xc40_experiment"
        version: "version_0"      # Optional version tag
        log_graph: true           # Whether to log model graph
        log_attention: false      # Whether to log attention weights (memory intensive)

    # Mixed Precision Training
    precision:
        mixed_precision: true     # Enable AMP (Automatic Mixed Precision)
        amp_level: "O1"           # Optimization level (O1, O2, O3)

    tokenizer:
        tokenizer_path: "tokenizer_state_xc40.txt"
        tokenizer_length: 64  # Maximum sequence length for tokenization
        vocab_size: