base_config:
    device: "cuda"

    # Model Hyperparameters
    model:
        vocab_size: 1825
        embed_dim: 512
        num_heads: 8
        num_layers: 6
        ffn_dim: 2048
        num_windows: 30  # Sliding window size for multi-head attention
        max_len: 512
        context_length: 1024  # The # steps in the past as input
        prediction_steps: 128  # The # steps in the future to predict
        num_groups: 8
        dropout: 0.1

    # Training Parameters
    training:
        num_epochs: 10
        batch_size: 600

    # Positional Encoding (Defined in event_transformer/models/position.py)
    rpe:
        rpe_class: False # Either False or String
    ape:
        ape_class: False # Either False or String

    # Encoder Settings (Defined in event_transformer/models/decoder.py)
    decoder:
        decoder_class: "MultiHeadAttention"  # Specify decoder class

    # Dataset Settings
    dataset:
        name: "xc40"
        path: "data/xc40/"
        class: "xc40_dataset" # Make sure to use the correct file name for your dataset without .py extension
        drain_path: "drain3_state_xc40.bin"
        vocab_path: "tokenizer_state_xc40.txt"

